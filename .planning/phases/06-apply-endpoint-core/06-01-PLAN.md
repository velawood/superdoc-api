---
phase: 06-apply-endpoint-core
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/utils/recompress.mjs
  - src/utils/apply-buffer.mjs
autonomous: true

must_haves:
  truths:
    - "recompressDocxBuffer(buffer) takes an uncompressed DOCX buffer and returns a smaller compressed buffer"
    - "applyEditsToBuffer(editor, edits, ir, options) applies validated edits to an editor and exports the result as a Buffer"
    - "applyEditsToBuffer handles all four edit operations: replace, delete, insert, comment"
    - "applyEditsToBuffer sorts edits in descending position order before applying"
  artifacts:
    - path: "src/utils/recompress.mjs"
      provides: "In-memory DOCX recompression via unzipper + archiver (level 9)"
      exports: ["recompressDocxBuffer"]
    - path: "src/utils/apply-buffer.mjs"
      provides: "Buffer-based edit application wrapper using existing domain modules"
      exports: ["applyEditsToBuffer"]
  key_links:
    - from: "src/utils/apply-buffer.mjs"
      to: "src/editApplicator.mjs"
      via: "imports sortEditsForApplication, validateEditsAgainstIR"
      pattern: "sortEditsForApplication"
    - from: "src/utils/apply-buffer.mjs"
      to: "src/blockOperations.mjs"
      via: "imports replaceBlockById, deleteBlockById, insertAfterBlock, addCommentToBlock"
      pattern: "replaceBlockById"
    - from: "src/utils/recompress.mjs"
      to: "archiver"
      via: "ZIP creation with level 9 compression"
      pattern: "archiver.*zip"
---

<objective>
Create the two utility modules needed by the apply endpoint: in-memory DOCX recompression and buffer-based edit application.

Purpose: These are the domain-integration building blocks for Phase 6. The recompress utility fulfills APPLY-03 (recompressed DOCX output). The apply-buffer utility adapts the existing file-path-based editApplicator workflow to work with in-memory buffers from HTTP uploads. Both are pure library modules with no route or plugin dependencies.

Output: Two new files in src/utils/ providing recompressDocxBuffer() and applyEditsToBuffer().
</objective>

<execution_context>
@/Users/alin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/alin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-apply-endpoint-core/06-RESEARCH.md

@src/editApplicator.mjs
@src/blockOperations.mjs
@src/editorFactory.mjs
@src/irExtractor.mjs
@superdoc-redline.mjs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create in-memory DOCX recompression utility</name>
  <files>src/utils/recompress.mjs</files>
  <action>
Create `src/utils/` directory and `src/utils/recompress.mjs`.

This utility extracts a DOCX buffer to in-memory file entries, then recompresses with archiver at level 9. This is the HTTP-adapted version of the CLI `recompress` command in superdoc-redline.mjs (lines 558-640) which uses temp files on disk. The HTTP version must be fully in-memory.

Implementation:

```javascript
import archiver from 'archiver';
import { Readable } from 'stream';
import { Open } from 'unzipper';
```

Export `async function recompressDocxBuffer(docxBuffer)`:

1. **Extract phase** - Use `Open.buffer(docxBuffer)` to read the ZIP central directory (same API used in checkZipBomb from file-upload.mjs). This returns a directory object with `files` array. For each file entry, call `entry.buffer()` to get its content. Collect into a `Map<string, Buffer>`.

   NOTE: Use `Open.buffer()` NOT `unzipper.Parse()` streaming. Open.buffer() is more reliable for in-memory work and we already use it in checkZipBomb (Phase 3). The streaming Parse() approach has edge cases with entry event ordering.

2. **Compress phase** - Create `archiver('zip', { zlib: { level: 9 } })`. Collect output chunks via `archive.on('data', ...)`. Create a promise that resolves on `archive.on('end')` with `Buffer.concat(chunks)`. Add each file from the Map with `archive.append(content, { name: path })`. Call `archive.finalize()`. Await and return the result buffer.

3. **Error handling** - If extraction or compression fails, throw with descriptive message. The caller (route handler) will catch and decide whether to fall back to uncompressed.

Add JSDoc documenting the function signature, purpose (SuperDoc exports uncompressed ZIP; this reduces ~6x), and that it throws on failure.
  </action>
  <verify>
Run `node -e "import('./src/utils/recompress.mjs').then(m => console.log(typeof m.recompressDocxBuffer))"` to verify module loads and exports the function.
  </verify>
  <done>
recompressDocxBuffer(docxBuffer) exported from src/utils/recompress.mjs. Uses Open.buffer() for extraction and archiver level 9 for compression. Fully in-memory, no temp files.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create buffer-based edit application wrapper</name>
  <files>src/utils/apply-buffer.mjs</files>
  <action>
Create `src/utils/apply-buffer.mjs`.

This wraps the existing domain module workflow (from editApplicator.mjs's `applyEdits` function, lines 273-430) to work with an already-loaded editor and buffer instead of file paths. The route handler will create the editor; this module applies edits and exports the result.

Import these from existing domain modules:
```javascript
import { sortEditsForApplication } from "../editApplicator.mjs";
import {
  replaceBlockById,
  deleteBlockById,
  insertAfterBlock,
  addCommentToBlock,
} from "../blockOperations.mjs";
import { isTocBlock, detectTocStructure } from "../editApplicator.mjs";
```

Export `async function applyEditsToBuffer(editor, edits, ir, options = {})`:

Parameters:
- `editor` - Already-loaded SuperDoc editor instance (in suggesting mode)
- `edits` - Array of validated edit objects (already passed validateEditsAgainstIR)
- `ir` - Document IR (from extractIRFromEditor) for position resolution
- `options.author` - Author object (default: `{ name: 'API User', email: 'api@superdoc.com' }`)

Implementation (follow the pattern from editApplicator.mjs applyEdits, lines 354-430):

1. Sort edits: `const sortedEdits = sortEditsForApplication(edits, ir)`

2. Create internal helper `resolveBlockId(blockId, ir)` that:
   - Looks up block by seqId first: `ir.blocks.find(b => b.seqId === blockId)`
   - Then by id: `ir.blocks.find(b => b.id === blockId)`
   - Returns the UUID (block.id) or null
   (This matches resolveBlockIdFromIR in editApplicator.mjs lines 439-453)

3. For each sorted edit, apply using a switch on `edit.operation`:
   - `replace`: call `replaceBlockById(editor, resolvedId, edit.newText, { diff: edit.diff !== false, trackChanges: true, author, verbose: false })`. If replace succeeds AND edit.comment exists, call `addCommentToBlock` and push to comments array.
   - `delete`: call `deleteBlockById(editor, resolvedId, { trackChanges: true, author })`
   - `insert`: call `insertAfterBlock(editor, resolvedId, edit.text, { type: edit.type || 'paragraph', level: edit.level, trackChanges: true, author })`. If insert succeeds AND edit.comment exists, add comment.
   - `comment`: call `addCommentToBlock(editor, resolvedId, edit.comment, author)` and push to comments array.

   Handle TOC block detection: Before replace operations, check `detectTocStructure(block)` on the IR block. If isToc, skip the edit (log warning, don't throw).

   Wrap each edit application in try-catch. If an edit fails, skip it and continue (individual edit failures after validation are operational, not validation errors).

4. Export document:
   - Build exportOptions: `{ isFinalDoc: false, commentsType: 'external' }`
   - If comments.length > 0, add `exportOptions.comments = comments`
   - Reset cursor: try `editor.commands.setTextSelection(1)` catch ignore (matches editApplicator pattern)
   - Suppress TextSelection warning: temporarily replace console.warn to filter "TextSelection endpoint not pointing into a node" messages (matches editApplicator pattern, lines 406-414)
   - Call `const exportedBuffer = await editor.exportDocx(exportOptions)`
   - Restore console.warn in finally block
   - Return `Buffer.from(exportedBuffer)`

5. Return value: The exported DOCX buffer (uncompressed -- recompression is done separately by the route handler)

Add comprehensive JSDoc documenting parameters, return type, and the assumption that edits have already been validated.
  </action>
  <verify>
Run `node -e "import('./src/utils/apply-buffer.mjs').then(m => console.log(typeof m.applyEditsToBuffer))"` to verify module loads and exports the function.
  </verify>
  <done>
applyEditsToBuffer(editor, edits, ir, options) exported from src/utils/apply-buffer.mjs. Sorts edits, resolves block IDs, applies all four operation types via existing domain modules, exports document buffer. Follows editApplicator.mjs patterns for TOC detection, cursor reset, and warning suppression.
  </done>
</task>

</tasks>

<verification>
1. `node -e "import('./src/utils/recompress.mjs').then(m => console.log('recompress:', typeof m.recompressDocxBuffer))"` - Module loads, function exported
2. `node -e "import('./src/utils/apply-buffer.mjs').then(m => console.log('apply-buffer:', typeof m.applyEditsToBuffer))"` - Module loads, function exported
3. `node --test tests_and_others/tests/server.test.mjs` - Existing tests still pass (no changes to existing files)
</verification>

<success_criteria>
- src/utils/recompress.mjs exports recompressDocxBuffer() using Open.buffer() + archiver level 9
- src/utils/apply-buffer.mjs exports applyEditsToBuffer() wrapping existing domain modules for buffer workflow
- Both modules load without errors
- No changes to existing source files (both are NEW files)
- Existing test suite unaffected
</success_criteria>

<output>
After completion, create `.planning/phases/06-apply-endpoint-core/06-01-SUMMARY.md`
</output>
